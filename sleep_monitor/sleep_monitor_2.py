import cv2
import dx_engine as dx
import numpy as np
import time
import json
import paho.mqtt.client as mqtt
import struct
import math
import os
from collections import deque, Counter # â˜… Counter ì¶”ê°€ (ë‹¤ìˆ˜ê²° íˆ¬í‘œìš©)

# ==========================================
# 1. ì„¤ì • (Settings)
# ==========================================
MODEL_PATH = "/home/orangepi/deepx_sdk/dx_app/assets/models/YOLOV5Pose640_1.dxnn"

MQTT_BROKER_HOST = "192.168.219.107"
MQTT_BROKER_PORT = 1883
MQTT_TOPIC = "sleep_monitor/user/test01"

LAYER_CONFIG = [
    {"stride": 8,  "anchor_width": [19.0, 44.0, 38.0], "anchor_height": [27.0, 40.0, 94.0]},
    {"stride": 16, "anchor_width": [72.0, 103.0, 187.0], "anchor_height": [92.0, 198.0, 141.0]},
    {"stride": 32, "anchor_width": [156.0, 237.0, 373.0], "anchor_height": [287.0, 397.0, 525.0]}
]

# â˜…â˜…â˜… ì„ê³„ê°’ ì„¤ì • â˜…â˜…â˜…
WEAK_THRESHOLD = 0.05       # íŒŒì‹±ìš©
CONF_THRESHOLD = 0.30       # ì–´ê¹¨/ëª¸í†µìš©
FACE_THRESHOLD = 0.70       # ì–¼êµ´ íŒë³„ ê¸°ì¤€
MIN_BOX_AREA = 5000         # ë…¸ì´ì¦ˆ í•„í„°

MOVEMENT_THRESHOLD = 10      # ë¯¼ê°ë„: 6í”½ì…€
CENTER_BUFFER_SIZE = 5      # ì›€ì§ì„ ë°˜ì‘ì†ë„

# â˜… [NEW] ìƒíƒœ ì•ˆì •í™” ë²„í¼ (30í”„ë ˆì„ = ì•½ 1ì´ˆ)
# ì´ ê°’ì„ ëŠ˜ë¦¬ë©´ ë” ì•ˆì •ì ì´ì§€ë§Œ ë°˜ì‘ì´ ëŠë ¤ì§€ê³ , ì¤„ì´ë©´ ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •í•¨
STATUS_BUFFER_SIZE = 30     

NOSE, L_EYE, R_EYE = 0, 1, 2
L_EAR, R_EAR = 3, 4
L_SHOULDER, R_SHOULDER = 5, 6
L_HIP, R_HIP = 11, 12 

SKELETON_PAIRS = [
    (0, 1), (0, 2), (1, 3), (2, 4),
    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),
    (5, 11), (6, 12), (11, 12),
    (11, 13), (13, 15), (12, 14), (14, 16)
]

center_history = deque(maxlen=CENTER_BUFFER_SIZE)
status_history = deque(maxlen=STATUS_BUFFER_SIZE) # ìƒíƒœ ê¸°ë¡ìš© í

# ==========================================
# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ==========================================
def on_connect(client, userdata, flags, rc):
    if rc == 0: 
        print(f"MQTT ë¸Œë¡œì»¤ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤ (Host: {MQTT_BROKER_HOST})")
    else: 
        print(f"âŒ MQTT ì—°ê²° ì‹¤íŒ¨ (Code: {rc})")

def open_camera_robust():
    cap = cv2.VideoCapture(0, cv2.CAP_V4L2)
    if not cap.isOpened(): cap = cv2.VideoCapture(0)
    for i in range(1, 5):
        if cap.isOpened(): break
        cap = cv2.VideoCapture(i, cv2.CAP_V4L2)
    return cap if cap.isOpened() else None

def clamp(val, min_val, max_val):
    return max(min_val, min(val, max_val))

def parse_yolov5_with_anchors(output, original_shape):
    detections = []
    img_h, img_w = original_shape[:2]
    try:
        if not isinstance(output, np.ndarray): return detections
        if output.dtype == np.uint8 and output.ndim == 3:
            _, num_dets, data_size = output.shape
            if data_size == 256: 
                for det_idx in range(num_dets):
                    det_bytes = output[0, det_idx, :].tobytes()
                    box_raw = np.frombuffer(det_bytes[0:16], dtype=np.float32)
                    grid_y, grid_x, anchor_idx, layer_idx = struct.unpack('4B', det_bytes[16:20])
                    conf = np.frombuffer(det_bytes[20:24], dtype=np.float32)[0]
                    
                    if conf < WEAK_THRESHOLD: continue
                    if layer_idx >= len(LAYER_CONFIG): continue
                    
                    cfg = LAYER_CONFIG[layer_idx]
                    stride = cfg["stride"]
                    aw, ah = cfg["anchor_width"][anchor_idx], cfg["anchor_height"][anchor_idx]
                    
                    xc = (grid_x - 0.5 + (box_raw[0] * 2)) * stride
                    yc = (grid_y - 0.5 + (box_raw[1] * 2)) * stride
                    w_model = (box_raw[2] ** 2) * 4 * aw
                    h_model = (box_raw[3] ** 2) * 4 * ah
                    
                    scale_x, scale_y = img_w / 640.0, img_h / 640.0
                    x1 = clamp(int((xc - w_model/2) * scale_x), 0, img_w)
                    y1 = clamp(int((yc - h_model/2) * scale_y), 0, img_h)
                    x2 = clamp(int((xc + w_model/2) * scale_x), 0, img_w)
                    y2 = clamp(int((yc + h_model/2) * scale_y), 0, img_h)
                    
                    kpts_floats = np.frombuffer(det_bytes[28:232], dtype=np.float32)
                    keypoints = []
                    for i in range(17):
                        kp_x = ((kpts_floats[i*3] * 2.0 - 0.5 + grid_x) * stride) * scale_x
                        kp_y = ((kpts_floats[i*3+1] * 2.0 - 0.5 + grid_y) * stride) * scale_y
                        kp_conf = 1.0 / (1.0 + np.exp(-kpts_floats[i*3+2]))
                        keypoints.append((kp_x, kp_y, kp_conf))
                    
                    detections.append({'bbox': (x1, y1, x2, y2), 'area': (x2-x1)*(y2-y1), 'keypoints': keypoints, 'confidence': float(conf)})
    except Exception: pass
    return detections

def get_body_center(detection):
    kpts = detection['keypoints']
    bbox = detection['bbox']
    hips = [kpts[i] for i in [L_HIP, R_HIP] if kpts[i][2] > CONF_THRESHOLD]
    if hips: return (sum(h[0] for h in hips)/len(hips), sum(h[1] for h in hips)/len(hips))
    shoulders = [kpts[i] for i in [L_SHOULDER, R_SHOULDER] if kpts[i][2] > CONF_THRESHOLD]
    if shoulders: return (sum(s[0] for s in shoulders)/len(shoulders), sum(s[1] for s in shoulders)/len(shoulders))
    return ((bbox[0]+bbox[2])/2, (bbox[1]+bbox[3])/2)

def calculate_distance(p1, p2):
    if not p1 or not p2: return 0
    return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)

# ==========================================
# 3. â˜… í•µì‹¬: í†µí•© íŒë³„ ë¡œì§ + ì•ˆì •í™” â˜…
# ==========================================

def get_max_head_conf(kpts):
    scores = [kpts[NOSE][2], kpts[L_EYE][2], kpts[R_EYE][2], kpts[L_EAR][2], kpts[R_EAR][2]]
    return max(scores)

def determine_posture_instant(detection):
    """
    ìˆœê°„ì ì¸ ìì„¸ íŒë³„ (1í”„ë ˆì„ìš©)
    """
    kpts = detection['keypoints']
    bbox = detection['bbox']
    
    head_conf = get_max_head_conf(kpts)  
    l_shoulder_conf = kpts[L_SHOULDER][2]
    r_shoulder_conf = kpts[R_SHOULDER][2]
    
    box_w = bbox[2] - bbox[0]
    box_h = bbox[3] - bbox[1]
    aspect_ratio = box_h / box_w if box_w > 0 else 0 
    
    has_strong_face = (head_conf > FACE_THRESHOLD)
    has_strong_body = (l_shoulder_conf > CONF_THRESHOLD and r_shoulder_conf > CONF_THRESHOLD)

    # [Case A] ì´ë¶ˆ ì•ˆ ë®ìŒ (ëª¸ì´ ì„ ëª…) -> ì •ë°€ ë¡œì§
    if has_strong_body:
        y_diff = abs(kpts[L_SHOULDER][1] - kpts[R_SHOULDER][1])
        if has_strong_face:
            if y_diff < 25: return "SIDE" # ë°˜ì „ ë¡œì§
            else: return "UPRIGHT"
        else:
            # ëª¸ì€ ë³´ì´ëŠ”ë° ì–¼êµ´ì´ ì•ˆ ë³´ì„ -> ì—ë“œë¦¼
            return "PRONE"

    # [Case B] ì´ë¶ˆ ì† (ëª¸ì´ í¬ë¯¸) -> ë­‰ëš±ê·¸ë ¤ íŒë‹¨
    else:
        # 1. ì–´ê¹¨ê°€ ë¶ˆì•ˆì •í•˜ë¯€ë¡œ ì ˆëŒ€ ì–´ê¹¨ ë†’ì´ë¥¼ ë¯¿ì§€ ì•ŠìŒ
        # 2. ëŒ€ì‹  ë°•ìŠ¤ ë¹„ìœ¨ê³¼ ì–¼êµ´ ìœ ë¬´ë¡œë§Œ íŒë‹¨
        
        # ë°•ìŠ¤ê°€ ì¢ê³  ëš±ëš±í•¨ (ë¹„ìœ¨ > 0.9) -> ì˜†ìœ¼ë¡œ ì›…í¬ë ¤ ì 
        if aspect_ratio > 0.9: 
            return "SIDE"
            
        # ë°•ìŠ¤ê°€ ë‚©ì‘í•¨ (ì¼ë°˜ì  ëˆ„ì›€)
        if has_strong_face:
            return "UPRIGHT"
        else:
            return "PRONE"

def get_stabilized_status(current_status):
    """
    [NEW] ìƒíƒœ ì•ˆì •í™” í•¨ìˆ˜ (íˆ¬í‘œ ì‹œìŠ¤í…œ)
    ìµœê·¼ Nê°œì˜ ìƒíƒœ ì¤‘ ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ìƒíƒœë¥¼ ë°˜í™˜
    """
    status_history.append(current_status)
    
    # ë°ì´í„°ê°€ ì•„ì§ ëœ ëª¨ì˜€ìœ¼ë©´ í˜„ì¬ ìƒíƒœ ë¦¬í„´
    if len(status_history) < 5:
        return current_status
        
    # ìµœë¹ˆê°’(ê°€ì¥ ë§ì´ ë‚˜ì˜¨ ìƒíƒœ) ì°¾ê¸°
    counter = Counter(status_history)
    most_common_status = counter.most_common(1)[0][0]
    return most_common_status

# ==========================================
# 4. ë©”ì¸ ì‹¤í–‰
# ==========================================
def main():
    print("ğŸš€ ìˆ˜ë©´ ëª¨ë‹ˆí„°ë§ (Stabilized Version)")
    
    client = mqtt.Client()
    client.on_connect = on_connect
    try: client.connect(MQTT_BROKER_HOST, MQTT_BROKER_PORT, 60); client.loop_start()
    except: pass

    if not os.path.exists(MODEL_PATH): print("âŒ ëª¨ë¸ ì—†ìŒ"); return
    ie = dx.InferenceEngine(MODEL_PATH)
    cap = open_camera_robust()
    if not cap: return
    
    cap.set(3, 640); cap.set(4, 480); cap.set(5, 30)
    
    prev_avg_center = None
    movement_counter = 0
    last_mqtt_time = time.time()
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret: break
            
            input_tensor = cv2.resize(frame, (640, 640))
            input_tensor = cv2.cvtColor(input_tensor, cv2.COLOR_BGR2RGB)
            input_bytes = np.array(input_tensor, dtype=np.uint8).tobytes()
            outputs = ie.run([np.frombuffer(input_bytes, dtype=np.uint8)])
            
            detections = parse_yolov5_with_anchors(outputs[0], frame.shape)
            # ë…¸ì´ì¦ˆ í•„í„° (ë¨¼ì§€ ì œê±°)
            valid_detections = [d for d in detections if d['area'] > MIN_BOX_AREA]
            
            # ê¸°ë³¸ê°’
            raw_status = "BED_EXIT"
            final_status = "BED_EXIT"
            status_color = (0, 0, 255)
            
            if valid_detections:
                valid_detections.sort(key=lambda x: x['area'], reverse=True)
                target = valid_detections[0]
                
                # 1. ìˆœê°„ íŒë³„
                raw_status = determine_posture_instant(target)
                
                # 2. â˜… ìƒíƒœ ì•ˆì •í™” (Voting) â˜…
                # ìˆœê°„ì ìœ¼ë¡œ íŠ€ëŠ” ê°’(ë…¸ì´ì¦ˆ)ì„ ê±¸ëŸ¬ë‚´ê³  ë‹¤ìˆ˜ê²°ë¡œ ê²°ì •
                final_status = get_stabilized_status(raw_status)
                status_color = (0, 255, 0)
                
                # --- ì›€ì§ì„ ê°ì§€ ---
                curr_raw_center = get_body_center(target)
                center_history.append(curr_raw_center)
                avg_x = sum(c[0] for c in center_history) / len(center_history)
                avg_y = sum(c[1] for c in center_history) / len(center_history)
                curr_avg_center = (avg_x, avg_y)

                if prev_avg_center:
                    dist = calculate_distance(prev_avg_center, curr_avg_center)
                    if dist > MOVEMENT_THRESHOLD:
                        movement_counter += 1
                        print(f"ë’¤ì²™ì„ ê°ì§€! (ëˆ„ì  : {movement_counter}íšŒ ), ì´ë™ ê±°ë¦¬ : {dist:.2f} í”½ì…€")
                
                prev_avg_center = curr_avg_center
                
                # --- ì‹œê°í™” ---
                x1, y1, x2, y2 = target['bbox']
                cv2.rectangle(frame, (x1, y1), (x2, y2), status_color, 2)
                
                head_score = get_max_head_conf(target['keypoints'])
                debug_text = f"Head: {head_score:.2f}"
                cv2.putText(frame, debug_text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

                kpts = target['keypoints']
                for pair in SKELETON_PAIRS:
                    if pair[0] < len(kpts) and pair[1] < len(kpts):
                        pt1 = kpts[pair[0]]
                        pt2 = kpts[pair[1]]
                        if pt1[2] > WEAK_THRESHOLD and pt2[2] > WEAK_THRESHOLD:
                            cv2.line(frame, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (0, 255, 0), 2)
                
                for i, kp in enumerate(kpts):
                    if kp[2] > WEAK_THRESHOLD:
                        color = (0, 0, 255) if i < 5 else (255, 0, 0)
                        cv2.circle(frame, (int(kp[0]), int(kp[1])), 4, color, -1)

            else:
                prev_avg_center = None
                center_history.clear()
                status_history.clear() # ì‚¬ëŒì´ ì—†ìœ¼ë©´ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”

            # í™”ë©´ì—ëŠ” 'ì•ˆì •í™”ëœ' ìµœì¢… ìƒíƒœ í‘œì‹œ
            cv2.putText(frame, f"STATUS: {final_status}", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)
            cv2.putText(frame, f"MOVES: {movement_counter}", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
            
            # --- MQTT ë°œí–‰ ---
            if time.time() - last_mqtt_time > 1.0:
                payload = {"status": final_status, "movements": movement_counter, "timestamp": time.time()}
                json_str = json.dumps(payload, ensure_ascii=False)
                client.publish(MQTT_TOPIC, json_str)
                print(f"MQTT ë°œí–‰ ì„±ê³µ : {json_str}")
                last_mqtt_time = time.time()
                
            cv2.imshow("Stabilized Sleep Monitor", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'): break
    finally:
        cap.release(); cv2.destroyAllWindows(); client.loop_stop()

if __name__ == "__main__":
    main()
# We-meet 보고서

![Monoculus는 Mono- (하나) + -oculus (눈) 의 합성어로, 1인 가구의 가구주 한 명만 바라보며 그의 안전을 지켜준다는 의미를 가진 시스템이다. ](attachment:18ab8fc0-36a7-4dfb-b3c2-dc8ad042dcc4:image.png)

Monoculus는 Mono- (하나) + -oculus (눈) 의 합성어로, 1인 가구의 가구주 한 명만 바라보며 그의 안전을 지켜준다는 의미를 가진 시스템이다. 

# [1] 프로젝트 배경

### **1.1. 1인 가구의 가파른 증가와 인구학적 다변화**

![image.png](attachment:4fdb92b0-473c-44b4-a813-a1bf3d1c1b90:image.png)

통계청의 「2024 통계로 보는 1인 가구」에 따르면, 대한민국의 1인 가구 비중은 2023년 기준 전체 가구의 **35.5%(782만 9천 가구)**를 차지하며 가장 주된 가구 형태로 자리 잡았다. 주목할 점은 1인 가구의 연령대별 분포가 **70세 이상(19.1%)**과 **29세 이하(18.6%)**에 집중되어 있다는 것이다. 이는 청년층의 독립 가구 증가와 고령층의 독거 노인 증가가 동시에 진행되고 있음을 시사하며, 이에 따라 **'범죄 예방(청년층/여성)'**과 **'건강 및 돌봄(고령층)'**이라는 상이한 두 가지 니즈를 동시에 충족하는 솔루션이 요구된다.

### **1.2. 주거 환경의 취약성과 화재 위험**

![image.png](attachment:3c54555f-9746-45a7-bbe8-17ff737e7b9e:image.png)

1인 가구의 주거 형태는 아파트(34.9%)보다 **단독주택(40.1%, 다가구 포함)**의 비중이 월등히 높다. 이러한 단독주택 밀집 지역은 아파트 단지에 비해 소방차 진입이 어렵거나, 스프링클러 등 자동 소화 설비가 미비한 경우가 많아 화재 발생 시 초기 대응에 매우 취약하다. 실제로 1인 가구는 다인 가구에 비해 화재 인지 및 대처 또한 늦어져 인명 피해로 이어질 가능성이 높으므로, 시각적 정보(불꽃, 연기)를 통한 조기 화재 감지 시스템이 필수적이다.

### **1.3. 범죄 불안과 개인정보 유출의 딜레마 (On-Device AI의 필요성)**

![image.png](attachment:01d83b72-578a-4be0-9c6b-9c2cf0bfc233:image.png)

1인 가구가 인식하는 **사회의 가장 큰 불안 요인은 '범죄(17.2%)'**로 나타나, 주거 침입 등에 대한 방범 수요가 매우 높다. 그러나 역설적으로 사용자가 일상에서 **가장 안전하지 않다고 체감하는 요인 1위는 '개인정보 유출(57.8%)'**이다. 이는 "나를 지키기 위해 설치한 카메라가 나의 사생활을 유출할 수 있다"는 불안감을 의미한다. 따라서 영상을 클라우드 서버로 전송하지 않고, **엣지 디바이스(Edge Device) 내부에서 모든 영상을 처리하고 폐기하는 '온디바이스 AI(On-Device AI)' 기술**만이 범죄 예방과 프라이버시 보호라는 두 마리 토끼를 잡을 수 있는 유일한 대안이다.

### **1.4. 1인 가구의 건강 이상 및 고독사 불안 증가**

![image.png](attachment:86d6fa33-05a0-44bb-b099-0019831ab6bf:image.png)

1인 가구는 위급 상황 시 신고 및 발견이 어렵다는 구조적 취약성을 지니고 있다. 급성 심정지, 혈압 이상 등 응급상황 발생 시 즉각적인 대응이 불가능하며, 특히 독거노인의 경우 주택 내 낙상 비율이 **63.5%**에 달하는 상황에서 낙상 사고 시 응급 대처가 필수적이다.

이러한 발견 및 구조 지연은 **고독사 위험**으로 직결된다. 골든타임을 놓쳐 고독사로 이어지고, 사후 발견이 지연되는 사례가 빈번하게 발생하고 있다. 실질적 위험뿐만 아니라 1인 가구의 고독사, 건강 이상에 대한 **심리적 불안감** 또한 확대되고 있다.

따라서 **쓰러짐/낙상 등 건강 이상을 조기 감지하고 신속히 신고**하여 골든타임을 확보함으로써, 생명을 구할 수 있을 뿐만 아니라 1인 가구가 느끼는 건강 위협에 대한 불안감을 완화할 수 있다.

### **1.5. 수면의 질에 대한 사회적 관심 증대와 능동적 환경 제어의 필요성**

![image.png](attachment:8d927241-ecb1-4c9b-8411-a60f6cd321cd:image.png)

![스크린샷 2026-01-04 053547.png](attachment:6f487113-ed4a-4fce-a059-ef2eadc8f6ce:스크린샷_2026-01-04_053547.png)

**수면 시장(SleepTech)**은 폭발적으로 성장하고 있다. 2023년 29조원 규모에서 2032년 130조원 수준으로 연평균 **18.2% 성장**이 예상되며, 설문조사 응답자의 **86.4%**가 평소 숙면에 어려움을 겪는다고 응답하였다. 이는 수면의 질 개선이 현대인의 보편적인 요구사항임을 보여준다.

수면 환경, 특히 **실내 온도**와 수면의 질 간의 관계는 매우 긴밀한 인과관계가 있다. 실제로 수면 중에 체온 변화가 많고 심부 체온이 낮아져야 수면이 유도되며, 부적절한 실내 온도는 수면의 질을 저하시키는 주요 요인으로 작용한다. 그러나 기존의 스마트 홈 시스템은 사용자가 직접 온도를 설정하거나, 단순히 시간대별로 온도를 조절하는 수동적 방식에 머물러 있다.

따라서 **사용자의 재실 여부 및 수면 상태(뒤척임 감지)**를 실시간으로 파악하여, 이에 따라 **IoT 홈어시스턴트(Home Assistant) 냉난방 시스템을 자동으로 가동**하는 능동적 환경 제어 시스템이 필요하다. 이는 사용자의 개입 없이 최적의 수면 환경을 유지함으로써 수면의 질 향상에 실질적으로 기여할 수 있다.

---

# [2] 프로젝트 목표

본 프로젝트는 1인 가구의 안전과 돌봄을 위해, 프라이버시 침해 없이 독립적으로 구동되는 **"NPU 기반 온디바이스 AI 홈 케어 시스템"** 개발을 목표로 한다. 고성능 NPU(DeepX)를 활용하여 실시간 영상 분석을 수행하며, 주거 안전(Security)과 생활 케어(Care) 기능을 통합 제공한다.

![image.png](attachment:239106b8-e31e-405a-b4d3-9ff5923d28f4:image.png)

### **2.1. 핵심 기능 목표**

**가. 실시간 객체 인식을 통한 재난 및 범죄 예방**

- **침입 탐지:** YOLO 기반의 객체 인식 모델을 통해 **사람(객체)의 존재 여부**를 실시간으로 감지하고, 내부 망에 연결된 디바이스(스마트폰, PC 등)를 통한 **사용자의 재실 여부** 정보를 결합하여 침입자를 판별한다. 두 가지 정보의 교차 검증을 통해 오탐을 줄이고 정확한 침입 탐지 알림을 제공한다.
- **화재 조기 감지:** 화재 발생 초기 단계의 **'불꽃'**과 **'연기'** 객체를 신속하게 인식하여, 대형 화재로 번지기 전 골든타임 내에 사용자에게 경고를 전달한다.

**나. 라이프 케어 (낙상 및 수면 모니터링)**

- **낙상 감지:** 고령층 또는 환자 1인 가구에서 발생할 수 있는 치명적인 **'낙상(쓰러짐)'** 상황을 Pose Estimation(자세 추정) 기술 등을 통해 감지하여 즉각적인 비상 알림을 수행한다.
- **수면 케어 (Sleep Care):** 사용자의 수면 중 **뒤척임(움직임)**을 실시간으로 감지하고, 이를 **IoT 홈어시스턴트(Home Assistant) 냉난방 시스템**과 연동하여 실내 온도를 자동으로 조정한다. 이를 통해 최적의 수면 환경을 능동적으로 제어하여 사용자의 수면의 질을 향상시킨다.

![▲[화재 예시]                  ▲[침입 예시]                  ▲[낙상 예시]            ▲[수면 자세 인식 예시]](attachment:af2a960b-aea8-4e1b-b5bc-a69ff7ece04b:image.png)

▲[화재 예시]                  ▲[침입 예시]                  ▲[낙상 예시]            ▲[수면 자세 인식 예시]

### **2.2. 기술적 목표**

**가. NPU 기반 고성능 추론 최적화**

- **고해상도 입력의 실시간 처리:** FHD급(1920x1080) 이상의 고화질 홈캠 영상이 입력되더라도, NPU의 하드웨어 가속을 통해 모델 입력 규격(640×640)으로 효율적인 전처리(Pre-processing) 및 리사이징을 수행하여 **실시간성(Real-time FPS)**을 보장한다.
- **모델 경량화 및 컴파일:** YOLOv7 등 고성능 객체 인식 모델을 DeepX NPU 전용 포맷(.dxnn)으로 최적화 컴파일하고, 양자화(Quantization)를 통해 메모리 사용량을 최소화하면서도 인식 정확도를 유지한다.

**나. 프라이버시 보호 아키텍처 구현**

- **데이터의 로컬 처리:** 모든 영상 데이터의 분석과 추론 과정은 외부 서버를 거치지 않고 엣지 디바이스 내부에서 완결된다.
- **비식별 정보 전송:** 외부 네트워크로는 민감한 영상 원본 대신, 화재/침입/낙상 등 이벤트가 발생했을 때에 한정하여 (1) 감지 사실, (2) 사건 발생 시간, (3) 사건 발생 시 사진 만을 전송하여 사생활 영상 유출을 최소화한다.

---

# [3] Model Methods & Result

## **3.1. 화재 감지,방범**

### **3.1.1. AI Model Architecture**

본 프로젝트는 화재 및 침입을 실시간으로 감지하기 위해 YOLOv7 기반의 객체 인식 모델을 설계하였다. 1인 가구의 안전을 위해 골든타임 내 신속한 대응이 필수적이므로, DeepX NPU를 활용한 온디바이스 추론 아키텍처를 채택하였다.

- Model Selection: Backbone 모델로 화재 감지를 위해 'flame(불꽃)'과 'smoke(연기)' 두 가지 클래스를 구분하는 **YOLOv7** 모델을 선정하였다.
    - **Backbone Selection**: YOLOv7은 ELAN(Efficient Layer Aggregation Network) 구조를 사용하여 파라미터 수 대비 높은 정확도와 추론 속도를 제공하므로, 실시간 화재 감지 백본으로 최적이라 판단하였다.
    - **침입 감지**: 실시간성을 극대화하기 위해 경량화된 **YOLOv8n (COCO)** 모델을 사용하여 'person' 클래스를 고속으로 검출한다.
- Dataset & Training: 가정 내 화재 상황을 반영한 커스텀 데이터셋(Home Fire Dataset)과 공개 화재 데이터셋을 병합(Merged Dataset)하여 다양한 실내/실외, 주간/야간 환경에서의 불꽃과 연기 패턴을 학습하였다. 100 epoch, batch size 16으로 학습한 **v7_merged_100epoch_16batch** 모델이 최종 배포 모델로 선정되었다.
- Hardware Optimization (DeepX NPU Dedicated): 클라우드 서버 통신은 네트워크 지연으로 인한 골든타임 손실과 개인 영상 유출 위험이 있다. 이를 해결하기 위해 DeepX NPU를 전용 가속 장치로 선정하고, **ONNX 포맷으로 변환 후 DXNN 형태로 포팅**을 수행하여 NPU에서 실행 가능하도록 하였다.

### **3.1.2. Custom Algorithm Flow**

정확한 화재 및 침입 감지를 위해 다음과 같은 순차적 알고리즘 흐름을 개발하였다.

- 화재 감지 및 다단계 대응 로직
    - 단일 프레임의 높은 신뢰도만으로는 오탐 가능성이 있으므로, **시간 기반 평균 신뢰도(Time-Averaged Confidence)** 메커니즘을 도입하였다. 최근 일정 시간 창(Time Window) 내 모든 프레임에서 감지된 신뢰도의 평균값을 계산하여, 이 평균값이 임계값을 초과하는 경우에만 실제 위협으로 판단한다. 인접 프레임 간 신뢰도 차이가 크지 않다는 특성을 활용하여, 지속적으로 나타나는 화재는 높은 평균 신뢰도를 유지하고, 순간적으로 오인식된 사물(콘센트, 전구 등)은 낮은 평균 신뢰도로 자동 필터링된다.
    - **평균 신뢰도에 따라 차등적인 알림 등급을 적용하여 (`config.json` 설정 기준)**:
        - **0.20 미만**: 무시 (Monitoring - Noise)
        - **0.20 이상 ~ 0.35 미만 (LOW)**: 주의 (Caution) - 조용한 시각적 알림
        - **0.35 이상 ~ 0.60 미만 (MEDIUM)**: 경고 (Warning) - 사용자 확인 유도
        - **0.60 이상 (HIGH)**: 위험 (Danger) - 긴급 알림 및 대피 권고

[Fire_Final_v1.mov](attachment:eb152ab7-2374-4b58-8a13-ccd0d1a69bbf:Fire_Final_v1.mov)

- 침입 판별 로직 (**Dual Engine** + **Robust Presence Scan**)
    
    ![test_image_intrusion_detected.jpg](attachment:ab98a271-6867-4a71-8704-8a42d1d784a2:test_image_intrusion_detected.jpg)
    
- **독립된 YOLOv8 엔진**: 화재 감지와 독립적으로 실행되는 YOLOv8 Nano 모델이 'person' 객체를 검출한다.
- **재실 여부 교차 검증**: 사람이 검출되면 즉시 `user_presence.py`의 **3단계 Robust Scan** (ARP 테이블 → UDP Knocking → ICMP Ping)이 동작하여 거주자의 재실 여부를 정밀 확인한다. (사람 검출 O + 재실 X = 침입)
    - **평균 신뢰도 ≥ 0.60**: 침입 경보 발령 (Alert Threshold)
- 이러한 다중 검증 로직을 통해 문 앞 배달원이나 창밖 지나가는 행인은 오탐 필터링되며, 실제 침입 상황만 정확히 포착한다.
    
    [Intruder_Final_v1.mov](attachment:b7d3f9ca-909f-41b4-be66-422fe9f4b166:Intruder_Final_v1.mov)
    

### **3.1.3. System Implementation**

최종 시스템 파이프라인은 [NPU 기반 객체 검출 → 시간 기반 평균 신뢰도 계산 → 다단계 알림 등급 판정 → 침입/화재 최종 판별 → 즉각 대응]으로 구성되었다.

- Real-Time NPU Inference: DeepX NPU 가속을 통해 FHD급 홈캠 영상에서도 프레임 저하 없이 실시간으로 불꽃/연기/사람 객체를 추적하였다. 특히 **ThreadPoolExecutor**를 활용한 병렬 추론 파이프라인을 구축하여, 화재(YOLOv7)와 침입(YOLOv8) 두 개의 독립된 NPU 모델을 동시에 실시간으로 구동하면서도 평균 추론 시간 15ms 이하를 달성하였다.
- High Accuracy Detection: 최종 배포 모델(v7_merged_100epoch_16batch)은 검증 데이터셋에서 mAP@0.5 78.24%를 달성하였으나, 더 중요한 것은 **학습되지 않은 실제 환경에서 오탐 없이 안정적인 화재 감지 성능**을 보였다는 점이다. Validation 지표가 높더라도 실제 배포 환경에서의 일반화 성능은 떨어질 수 있음을 실증하였으며, 병합 데이터셋 학습과 실전 검증을 통해 다양한 조명 조건, 촬영 각도, 화재 단계에서 신뢰할 수 있는 감지율과 낮은 오탐률을 확보하였다.
- Privacy-Preserving Architecture: 평상시에는 영상이 외부로 전송되지 않으며, **오직 화재나 침입과 같은 실제 위협이 감지되었을 때만** 해당 시점의 이미지를 **HTTPS(TLS) 암호화 채널**을 통해 안전하게 전송하도록 설계하였다. 이를 통해 24시간 감시의 보안 효과는 누리면서도, 개인의 사생활이 담긴 영상이 불필요하게 서버에 저장되거나 유출될 위험을 **최소화**하여 **사용자 편의성과 개인정보 보호의 균형**을 극대화하였다.

## 3.2. 낙상 및 쓰러짐 탐지

### 3.2.1. AI Model Architecture

 낙상 및 쓰러짐 탐지를 수행하기 위해 머리의 하강 속도와 자세를 분석하는 방식을 채택하였으며, 이를 구현하고자 backbone 모델로 **YOLOv5-Pose** 를 사용하였다.  

- Algorithm: 단순히 넘어진 자세만을 판단하는 정적 모델은 취침 상태와 낙상을 구분하기 어렵다는 한계가 있다. 이를 극복하기 위해 **1단계**에서 **YOLOv5-Pose**(NPU)를 통해 인체 관절 좌표를 실시간으로 추출하고, **2단계**(CPU)에서 머리 좌표의 수직 가속도와 자세를 계산 및 분석하였다.
- Hardware Optimization (DeepX NPU Dedicated): 기존의 클라우드 서버 통신 방식은 네트워크 지연으로 인해 **골든타임을 놓칠 위험**이 있으며, 외부로 영상을 전송하는 과정에서 **사생활 유출 우려**가 있다는 명확한 한계가 있다. 이러한 문제를 해결하기 위해 본 시스템은 **DeepX NPU**를 메인 연산 장치로 채택하였다. 특히 **DXNN 형태로 포팅된 모델 파일**을 활용하여 NPU의 고속 병렬 연산을 수행하여, 외부 통신 없이 기기 자체(엣지)에서 실시간 낙상 판별이 가능하도록 구현하였다.

### 3.2.2. Custom Algorithm Flow

정확한 낙상 판별과 오작동 최소화를 위해 다음과 같은 순차적 알고리즘 흐름을 개발하였다.

- Step 1: 객체 검증
    - YOLOv5-Pose가 추출한 객체 중 ‘person’ class 에 대한 Confidence Score(신뢰도)가 0.5 이상인 객체만을 추적 대상으로 한정하였다.
- Step 2: 머리 추적 및 가속도 분석
    - 민감도를 높이기 위해 머리(Head) 위치를 최우선으로 추적한다. 코(Nose), 눈(Eyes), 귀(Ears) 순서의 계층적 우선순위 로직을 적용하여 머리 좌표를 특정하고, 해당 좌표의 Y축 변화 속도(Velocity)가 임계값(Threshold, 1200px/s)을 초과하며 급격히 하강할 때를 낙상 의심 구간으로 1차 판별한다.
- Step 3: 기하학적 자세 분석
    - 가속도만으로는 감지하기 어려운 '스르르 쓰러지는' 상황을 보완하기 위해 신체 비율을 분석한다. 서 있을 때의 Aspect Ratio(머리-허리 길이 대 어깨 너비 비율)가 1.5 이상에서 0.65 미만으로 급격히 역전되거나, 가로 면적(Horizontal Extent)이 비정상적으로 확장(Sprawl)되는 경우를 낙상으로 확정하여 오탐률을 낮췄다.

### 3.2.3. **System pipeline**

최종 시스템 파이프라인은 [NPU 기반 포즈 추출 → 하이브리드(가속도+자세) 추론 → 즉각적 대응] 이다. 

- Real-Time NPU Inference: DeepX NPU 가속을 통해 고해상도 영상에서도 프레임 저하 없이 다중 객체의 관절과 움직임을 실시간으로 추적했으며, CPU 처리 대비 압도적인 반응 속도를 확보하여 즉각적인 낙상 사고 인지가 가능했다.
- Active Alert System: 낙상 판별 즉시 애플리케이션의 화면에 경보를 울려 초기 대응을 유도함과 동시에 낙상 사진을 첨부하여 오탐지를 빠르게 파악할 수 있게 하여 신뢰도를 높였다.

### 3.2.4. 실행 결과

- **NPU 모니터 내에서의 실행 결과**
    1. 누워 있는 경우 : 낙상을 인식하지 않음. 

![스크린샷 2025-12-01 184927.png](attachment:04d7615f-cbdf-4039-93b0-81ce4548e6f0:스크린샷_2025-12-01_184927.png)

1. 낙상하는 경우 : 낙상으로 성공적으로 인식함. 

![스크린샷 2025-12-01 185115.png](attachment:7c719dac-9e0b-4462-a219-3995754ca500:스크린샷_2025-12-01_185115.png)

![스크린샷 2025-12-01 185153.png](attachment:ba72ab63-fdbb-4cdd-9ab6-a5a9fc6eb175:스크린샷_2025-12-01_185153.png)

[fall_detection (online-video-cutter.com).mp4](attachment:802b964d-d8b1-4ade-a0b0-8d549a12d325:fall_detection_(online-video-cutter.com).mp4)

- **애플리케이션 실행 결과**
    - 낙상 탐지가 수행될 때 (1) 모델 로그(VS code), (2) NPU 에서 실시간으로 처리하는 영상, (3) 휴대폰, (4) 웹사이트 에서 확인할 수 있는 바는 아래 영상과 같다.
        
        낙상 탐지 모델은 실시간으로 관절의 좌표 및 낙상 여부를 판단하며, NPU 영상은 이를 시각적으로 보여준다. 
        
        사용자 및 낙상 위험 1인가구의 보호자가 실제로 사용하는 애플리케이션에서는 평상시에는 경보가 없고 Home monitoring 이 수행되고 있음을 보여주며, 낙상 사고 발생 시 탐지 사진과 함께 경보를 제공한다. 모든 영상을 제공하지 않고, 사진 만을 전송하여 경보에 대한 신뢰성은 높이되, 개인정보 유출은 최소화하도록 하였다. 
        
        낙상 사고 사진과 발생 시각을 메인 화면 및 로그 확인 화면에서 확인가능하도록 설정하여 알림을 놓치더라도 재확인이 가능하다.
        
        [Fall_Final_v1.mov](attachment:2d02ceb7-391f-4f6f-bd2a-81e2ae94d826:Fall_Final_v1.mov)
        

## 3.3. 수면 관리

### 3.3.1 AI Model Architecture

본 프로젝트는 1인 가구의 수면 상태를 실시간으로 모니터링하기 위해 YOLO-v5-Pose 모델을 선정하고, DEEPX SDK를 적극 활용하여 시스템을 구축하였다.

- Model Selection : 객체 탐지(Object Detection)와 자세 추정(Pose Estimation)을 동시에 수행하는 Single Stage 모델인 YOLOv5-Pose를 채택하였다. 이는 DX-M1 NPU 아키텍처에 맞춰 양자화 및 최적화가 완료된 바이너리이며, 엣지 디바이스 환경에서도 높은 FPS와 정확도를 보장하는 효율적인 모델이다.

### 3.3.2 Custom Algorithm Flow

NPU가 출력한 17개 관절 좌표(Keypoints)를 유의미한 수면 정보로 변환하기 위해 다음과 같은 자체 개발 알고리즘을 적용하였다.

- Data Parsing : NPU의 Raw Tensor 출력에서 사람 객체별 17개 관절의 (x, y, confidence) 좌표를 파싱하였다.
- Posture Classification (자세 판별) : 상단 카메라 뷰의 특성을 고려하여, X축 너비가 아닌 ‘양 어깨의 수직(Y축) 높이 차이’를 계산하고, 이를 임계값(Threshold)와 비교하여 UPRIGHT(정자세)/SIDE(측면)/PRONE(엎드림)을 정밀하게 판별하였다.
- Movement Detection (뒤척임 감지) : 관절의 미세한 떨림을 배제하기 위해 4개 핵심 관절(어깨, 엉덩이)의 평균점인 ‘몸통 중심점’을 계산하고, 이전 프레임과의 이동 거리가 임계값(15px)을 초과할 경우에만 유효한 뒤척임으로 카운트하였다.
    
    ![스크린샷 2025-12-03 001620.png](attachment:e4cfb01d-a910-4cc6-9e4a-b1379b0e5707:스크린샷_2025-12-03_001620.png)
    

### 3.3.3 **System pipeline**

수면관리 시스템은 [감지 → 분석 → 전송 → 제어]로 구성되며, 4단계 파이프라인을 성공적으로 수행하였다.

- Latency : 사전 최적화된 .dxnn 모델을 사용하여 NPU 추론 속도를 극대화하였으며, 로컬 네트워크 통신을 통해 전체 시스템 지연 시간을 최소화하였다.
- Connectivity : Home Assistant가 MQTT 브로커를 통해 수면 데이터를 수신하고, 설정된 자동화 규칙(빈도 기반 온도 조절)이 정확한 타이밍에 트리거됨을 확인하였다.
    
    ![스크린샷 2025-12-03 001824.png](attachment:369b4295-02de-4370-99f3-52909814c617:스크린샷_2025-12-03_001824.png)
    

---

### 3.3.4 실행 결과

[Sleep_Final_v1.mov](attachment:720f33bd-e14e-4669-b323-a6d35c777b3e:Sleep_Final_v1.mov)

- 애플리케이션 실행 결과

본 시연 영상은 NPU 엣지 컴퓨팅과 Home Assistant 연동을 통한 지능형 수면 관리 시스템의 동작 과정을 보여준다.

1. Demo Configuration
    
    실제 환경에서는 입면 후 5분 뒤 수면 모드, 1시간 뒤 숙면 모드로 진입하나, 본 영상에서는 기능 검증을 위해 각 단계를 **10초 간격**으로 빠르게 전환되도록 설정하였다. (시간 설정은 사용자 수면 패턴에 맞춰 커스터마이징 가능)
    
2. 동작 시나리오 및 온도 제어 알고리즘
    
    본 시스템은 **정밀 수면 분석**을 통해 사용자가 ‘숙면(DEEP_SLEEP)’ 상태일 때의 움직임만을 유의미한 데이터로 수집하며, 이에 맞춰 단계별 온도 제어를 수행한다.
    
    **초기 상태 (21°C)**: 사용자가 침대에 누우면 쾌적한 입면을 돕기 위해 21**°**C로 시작
    
    **숙면 모드 전환 (23°C)**: 사용자가 깊은 잠(DEEP_SLEEP)에 들면 체온 유지를 위해 시스템이 자동으로 23**°**C로 온도를 높임
    
    **수면 케어 동작 (22°C)**: 숙면 중 **뒤척임이 3회 이상 감지**되면, 수면 환경이 덥다고 판단하여 온도를 1**°**C 낮춰 뒤척임을 완화하고 수면 질 개선
    
3. 시스템 아키텍처 및 Data flow
    1. **분석 및 전송 (NPU → HA)**
        - NPU가 카메라 영상을 통해 사용자의 수면 자세와 상태를 실시간으로 분석
        - 분석된 메타데이터는 MQTT 프로토콜을 통해 Home Assistant 서버로 전송
    2. **자동화 트리거 (Home Assistant)**
        - 수신된 데이터를 바탕으로 HA의 자동화 엔진이 작동
        - 예: “숙면 상태에서 뒤척임 빈도가 높음” 조건을 감지하여 에어컨 온도를 조절하는 액션 수행
    3. **피드백 및 동기화 (HA → NPU → App)**
        - HA가 처리한 결과(온도, 수면 모드, 상태 메시지 등)는 다시 MQTT를 통해 NPU로 회신
        - NPU는 이 데이터를 받아 app 화면에 즉시 동기화하여 사용자에게 현재 상황을 시각화

## **3.4. 시스템 통합**

본 프로젝트는 개별 기능(화재/방범, 낙상, 수면)을 단일 시스템으로 통합하고, 유지보수성과 확장성을 확보하기 위해 전체 코드를 모듈화하였다. `integrated_monitor_2.py`에서 시작된 통합 로직은 최종적으로 5개의 독립 모듈(`main.py`, `firedetector.py`, `intruderdetector.py`, `sleepmonitor.py`, `falldetector.py`)로 재구성되어 유기적으로 동작한다.

### **3.4.1. System Flow (Main Logic)**

시스템의 전체 흐름은 User Presence(재실 여부)에 따라 **HOME Mode**와 **AWAY Mode**로 자동 전환되며, 각 모드에 최적화된 NPU 모델을 동적으로 로드하여 실행한다.

1. **Initialization & Configuration**:
    - `main.py`가 실행되면 `config.json`을 로드하여 임계값 및 모델 경로를 설정한다.
    - `SystemManager` 클래스가 각 서브 모듈(`FireDetector`, `SleepMonitor` 등)을 초기화하고, `DataPusher`와 `RobustPresenceDetector`를 구동한다.
2. **Mode Switching & Model Loading**:
    - **Robust Presence Scan**: 백그라운드 스레드가 ARP/UDP/ICMP 3단계 스캔을 수행하여 사용자의 재실 여부를 실시간 확인한다.
    - **Dynamic Model Loading**:
        - **HOME Mode**: 사용자가 집에 있을 때. `YOLOv5-Pose` 모델을 로드하여 낙상 및 수면 상태를 분석한다.
        - **AWAY Mode**: 사용자가 외출했을 때. `YOLOv8n` (침입 감지)과 `YOLOv7` (화재 감지) 모델을 로드하여 보안 감시를 수행한다.
3. **Inference & Parallel Processing**:
    - FHD급 영상을 `640x640`으로 전처리하여 NPU에 전달한다.
    - **Parallel Pipeline**: AWAY 모드에서는 화재와 침입 감지라는 두 가지 무거운 작업을 동시에 수행해야 한다. 이를 위해 Python의 `ThreadPoolExecutor`를 사용하여 멀티스레드 환경에서 두 NPU 엔진에 대한 추론을 동시에 요청함으로써, 단일 스레드 방식의 순차적 대기(Blocking) 시간을 제거하여 전체 파이프라인의 처리 효율을 극대화하였다. (로그 상 Overlap Ratio로 병렬 효율 확인 가능)
4. **Action & Visualization**:
    - 감지된 결과(화재, 침입, 낙상 등)는 각 Detector 모듈의 `process()` 메서드를 통해 분석된다.
    - 위협 발생 시 `MQTT`를 통해 Home Assistant로 이벤트를 발행(Publish)하고, `HTTPS`로 연결된 Web Backend로 데이터를 전송하여 대시보드에 즉각 반영한다.

### **3.4.2. Modularization Strategy**

기존의 단일 파일(`integrated_monitor_2.py`)이 가진 복잡성을 해소하기 위해 다음과 같이 기능을 분리하였다.

- **`main.py`**: 시스템의 진입점(Entry Point). 전체 수명 주기 관리, 모드 전환, NPU 엔진 로딩 및 스레드 풀 관리를 담당한다.
- **`firedetector.py`**: 화재/연기 감지 전용 클래스. 시간 기반 평균 신뢰도 계산 및 알림 등급(LOW/MEDIUM/HIGH) 판정 로직을 포함한다.
- **`intruderdetector.py`**: 침입자 감지 전용 클래스. 시간 기반 필터링 및 쿨다운 로직을 처리한다.
- **`sleepmonitor.py`**: 수면 자세(Upright/Side/Prone) 판별 및 뒤척임 카운트, 일일 리셋 로직을 담당한다.
- **`falldetector.py`**: 머리 하강 속도 및 관절 비율(Aspect Ratio) 분석을 통한 낙상 감지 알고리즘을 수행한다.

이러한 모듈형 아키텍처는 특정 기능(예: 화재 감지 임계값 조정) 수정 시 다른 기능에 영향을 주지 않아 유지보수가 용이하며, 향후 새로운 AI 기능을 추가하기에도 유연한 구조를 갖추고 있다.

### **3.4.2. 최종 시스템 시연**

[스파게티 요리사들 시연.mov](attachment:2b6441c3-6f41-4540-953b-eb2a8e808d53:스파게티_요리사들_시연.mov)

# [4] Trouble shooting

## 4.1. 화재 감지 및 방범

### **4.1.1. 화재 오탐 감소 및 과적합 방지**

- **Problem** :
    - **일상 사물 오인식**: 초기 모델(v7_original_200epoch_16batch)에서 콘센트, 전구, 전열기 등 밝은 빛을 내는 일상적인 사물을 불꽃(flame)으로 오탐하는 False Positive 문제가 발생했다. 특히 벽면 콘센트의 LED 표시등이나 전자기기의 전원 램프를 화재로 잘못 판단하는 경우가 자주 발생했다.
    - **제한적인 학습 데이터**: 단일 데이터셋(Home Fire Dataset)만으로 학습하여 다양한 조명 조건, 촬영 각도, 화재 단계(초기/중기/말기)에 대한 일반화 성능이 부족했다.
    - **과적합(Overfitting) 발견**: 200 epoch까지 학습한 모델은 Validation set에서 mAP@0.5 = 0.9531로 매우 높은 성능을 보였으나, 실제 환경(학습되지 않은 동영상)에서 테스트 시 콘센트를 불꽃으로 오탐하는 문제가 발생했다. 반면 100 epoch 모델은 Validation mAP@0.5 = 0.7824(우측)로 낮았지만 실제 환경에서 오탐이 없었다.
    
    ![PR_curve.png](attachment:927ea51c-b8f4-44e5-93c5-6b2d3281c5a2:PR_curve.png)
    
    ▲  200 epoch, 16 Batch (mAP@0.5 = 0.9531)
    
    ![PR_curve.png](attachment:893d660a-914d-4129-b5d4-be263ba0e73f:PR_curve.png)
    
    ▲ 100 epoch, 16 Batch (mAP@0.5 = 0.9531)
    
    - **과적합의 원인**:
        1. **데이터셋 분포 편향**: 학습/검증 데이터는 주로 "화재 장면"만 포함하여 모델이 "밝은 빛 = 불꽃"이라는 과도한 상관관계를 학습했다. 실제 가정 환경에는 콘센트 LED, 전구, 가전제품 등 "비화재 밝은 빛"이 존재하지만, 이러한 케이스가 학습 데이터에 부족했다.
        2. **Validation Set의 한계**: Train/Val/Test 모두 같은 "화재 데이터셋"에서 분할되어 동일한 분포를 공유한다. 따라서 Validation 성능이 높아도 실제 배포 환경(일상 가정)에서의 일반화 성능을 보장하지 못한다.
        3. **과학습(Overlearning)**: 200 epoch에서 모델이 화재의 본질적 특징(불규칙한 형태, 움직임, 색상 변화)보다 데이터셋 특유의 패턴(특정 밝기, 특정 배경)을 과도하게 학습하여 실전에서 일반화 실패를 보였다.
- **Solution** :
    1. **데이터셋 확장 및 병합(Merged Dataset)**: 가정 내 화재 데이터셋(Home Fire Dataset)과 공개 화재 데이터셋을 병합하여 학습 데이터의 다양성을 확보했다. 이를 통해 실내/실외, 주간/야간, 다양한 배경과 조명 조건에서의 불꽃과 연기 패턴을 학습할 수 있었다.
    2. **실전 검증 시험 설계 (yolov7_video_compare.py)**: Validation 지표만으로는 실제 성능을 예측할 수 없다는 교훈을 바탕으로, 학습에 사용되지 않은 실제 화재 동영상(bucket11.mp4, printer31.mp4, roomfire41.mp4)을 별도로 확보하여 두 모델(100 epoch vs 200 epoch)의 실전 성능을 직접 비교했다.
        - **목적**: 두 모델 모두 동일한 미학습 데이터에서 화재와 연기를 탐지하는 양상을 관찰하여, 신뢰도 분포와 오탐 가능성(콘센트, 전구 등 일상 사물)을 정성적으로 검증
        - **방법**: 각 동영상에 대해 두 모델의 결과 동영상을 생성하고, 프레임별 Detection 수, 평균 신뢰도, 오탐 케이스를 육안 및 정량 분석
        
        ![printer31_v7_merged_200epoch_16batch.mp4_20251202_152605.422.jpg](attachment:5c6dfc42-0ca8-4443-9e08-bd56795a7fcc:printer31_v7_merged_200epoch_16batch.mp4_20251202_152605.422.jpg)
        
        ▲ 200 epoch, 16 Batch : 콘센트를 85% 신뢰도로 화재로 인식
        
        ![printer31_v7_merged_100epoch_16batch.mp4_20251202_152603.372.jpg](attachment:439e3300-7a18-4ae2-b915-9f2bac63aaac:printer31_v7_merged_100epoch_16batch.mp4_20251202_152603.372.jpg)
        
        ▲ 100 epoch, 16 Batch : 완전히 동일한 프레임에서 콘센트를 화재로 인식하지 않음 (신뢰도 < 50%)
        
        - **핵심 발견**: 200 epoch 모델이 Validation mAP는 높지만(95.31%), 실제 환경에서 콘센트를 0.85 신뢰도로 불꽃 오탐. 100 epoch 모델은 mAP는 낮지만(78.24%) 오탐 전무
    3. **실전 우수 모델 선정**: **v7_merged_100epoch_16batch**를 최종 모델로 채택했다. Validation 지표가 높더라도 실제 배포 환경에서의 일반화 성능은 떨어질 수 있으므로, 반드시 학습되지 않은 실제 데이터로 검증해야 한다는 원칙을 확립했다.
- **Result** :
    - **실전 우수 모델 선정**: 최종 모델로 **v7_merged_100epoch_16batch**를 채택하였다. Validation mAP@0.5는 78.24%로 200 epoch 모델(95.31%)보다 낮지만, 실제 환경에서 콘센트/전구 등 일상 사물에 대한 오탐이 없고, 화재 감지 성능은 200 epoch과 유사하여 실용성이 훨씬 높다.
    - **과적합 vs 일반화 교훈**: 단순히 Validation 지표만 보고 모델을 선택하면 과적합된 모델을 선택할 위험이 있다. 특히 화재 감지처럼 Real-world 환경이 학습 데이터와 크게 다를 수 있는 경우, 반드시 실제 환경에서 직접 테스트하여 오탐률을 확인해야 한다.
        - **성능 비교**:
    
    | 지표 | v7_merged_100epoch | v7_merged_200epoch | 최종 선택 |
    | --- | --- | --- | --- |
    | **Val mAP@0.5** | 0.7824 | 0.9531 | - |
    | **콘센트 오탐** | ❌ 없음 | ⚠️ 발생 (0.75+ 신뢰도) | **100 epoch** |
    | **실제 화재 감지** | ✅ 양호 | ✅ 양호 | 동등 |
    | **일반화 능력** | ✅ 높음 | ❌ 낮음 (과적합) | **100 epoch** |
    - **배포 전략**: 높은 Validation 점수보다 낮은 오탐률이 사용자 신뢰도에 더 중요하므로, 일반화 능력이 검증된 v7_merged_100epoch_16batch를 최종 배포 모델로 확정하였다.

### **4.1.2. NPU 포팅 및 추론 최적화 문제**

- **Problem (3단계 - 변환, 컴파일, 추론 단계별 문제)** : 이 문제는 단순한 포팅 작업이 아니라, **"변환(Conversion) - 컴파일(Compilation) - 추론(Inference)"의 3단계 전 과정**에서 각각 치명적인 오류가 발생한 복합적인 난제였다.
    1. **변환 단계 (Model Architecture Mismatch)**: YOLOv7의 기본 출력 구조는 FPN(Feature Pyramid Network)의 3개 스케일(P3, P4, P5)이 분리되어 출력된다. 그러나 NPU 컴파일러(DXNN)의 `SURGERY` 단계는 이러한 다중 출력을 단일 그래프로 해석하지 못해 변환을 거부했다.
    2. **컴파일 단계 (Compiler Version Trap)**: "최신 버전이 가장 좋다"는 일반적인 통념을 따라 최신 `dx_com v2.0.0`을 사용했으나, 타겟 장비(Orange Pi)의 런타임(DXRT v2.9.5)과 호환되지 않는 바이너리를 생성하여 `Unwanted Data Type`이라는 모호한 에러와 함께 로드 자체가 불가능했다.
    3. **추론 단계 (Signal Distortion)**: 어렵게 포팅에 성공한 초기 모델(Float32)조차 NPU 상에서는 모든 객체의 신뢰도(Confidence)가 **0.18**로 고정되거나, 활성화 함수 변경(LeakyReLU) 시 **1.0**으로 포화(Saturation)되어 화재와 비화재를 전혀 구분할 수 없었다. 이는 NPU가 Float32 연산보다 INT8 양자화 연산에 더 최적화되어 있다는 하드웨어 특성을 간과한 결과였다.
- **Solution (3단계 - 변환, 컴파일, 추론 단계별 문제)** :
    1. **출력 레이어 재설계 (Architectural Surgery)**: 모델의 끝단(Head)을 수정하여 3개의 스케일 출력을 하나의 거대한 텐서(`[1, 25200, 7]`)로 통합(Concatenation)했다. 이를 통해 NPU 컴파일러가 모델 전체를 하나의 최적화된 서브그래프로 인식하게 만들었다.
    2. **Downgrade for Compatibility**: 타겟 런타임(DXRT)의 릴리즈 노트를 역추적하여, 최신 컴파일러 대신 가장 호환성이 높은 **v1.60.1** 버전을 찾아내어 적용했다.
    3. **Opset 12 & INT8 전략 (Golden Example 분석)**: NPU 디바이스에 기본 포함된 예제 파일 `YOLOV7-2.onnx`가 정상 작동하는 것을 확인하고, 해당 모델의 메타데이터를 역분석했다. 그 결과 예제 모델이 **Opset 12**와 **INT8 Quantization**으로 구성되어 있음을 발견하고, 이를 우리 모델에 그대로 적용함으로써 SiLU 활성화 함수의 비선형성 문제를 완벽하게 해결했다.
- **Result (Quantifiable Breakthrough)** : 단순한 문제 해결을 넘어, 성능과 기능을 동시에 업그레이드하는 결과를 얻었다.
    
    
    | 항목 | 초기 시도 (Float32/SiLU) | 최적화 후 (INT8/Golden Path) | 개선 효과 |
    | --- | --- | --- | --- |
    | **추론 속도** | 63ms | **51ms** | **~19% 속도 향상** |
    | **신뢰도 분포** | 0.18 (고정) | **0.0 ~ 0.95 (정상)** | **탐지 기능 회복** |
    | **메모리 효율** | 낮음 | **높음 (Quantized)** | **최적화** |
    | **기능 확장** | 단일 클래스 | **Fire & Smoke (2-Class)** | **다중 탐지 성공** |
    - **최종 성과**: 이 최적화를 통해 엣지 디바이스(Orange Pi)에서도 **51ms(약 20FPS)**의 속도로 **화재와 연기를 동시에 정밀하게 탐지**할 수 있는 고성능 NPU 파이프라인을 구축했다. 이는 임베디드 AI 개발에서 SW(모델)와 HW(NPU)의 최적화 궁합(`Opset`+`Quantization`)이 얼마나 중요한지를 증명하는 사례다.

### **4.1.3. 침입 감지 시 일부 기기 미탐지 문제**

- **Problem** :
    - **ARP 응답 누락**: 초기 침입 탐지 시스템에서 네트워크 내 장치를 감지하기 위해 ARP(Address Resolution Protocol) 스캔을 사용했으나, 일부 스마트폰과 IoT 기기가 ARP 요청에 응답하지 않는 문제가 발생했다.
    - **절전 모드 및 프라이버시 기능**: 특히 iOS 기기와 일부 Android 기기는 배터리 절약을 위한 절전 모드나 MAC 주소 랜덤화(Privacy) 기능으로 인해 ARP 테이블에 나타나지 않거나, 응답이 간헐적으로 누락되었다.
    - **거주자 오탐**: 사용자가 집에 있음에도 불구하고 스마트폰이 ARP 스캔에서 감지되지 않아, 시스템이 "재실 없음"으로 판단하여 침입 경보를 발생시키는 오작동이 있었다.
- **Solution** :
    1. **Scapy 기반 직접 ARP 스캔**: 기존의 OS 수준 ARP 캐시 조회(`arp -a`) 방식에서 벗어나, Python scapy 라이브러리를 사용하여 브로드캐스트 ARP 요청을 직접 전송하고 응답을 수집하는 방식으로 변경했다.
    2. **재시도 및 타임아웃 증가**: 단일 스캔으로 놓칠 수 있는 기기를 포착하기 위해, ARP 요청의 재시도 횟수를 3회로 증가시키고 타임아웃을 2배로 연장(4초)하여 느리게 응답하는 기기도 감지할 수 있도록 했다.
    3. **신뢰 장치 다중 등록**: 단일 기기에 의존하지 않고, 사용자의 스마트폰, 태블릿, 스마트워치 등 여러 개인 기기를 신뢰 목록에 등록하여, 그 중 하나라도 감지되면 재실로 판단하는 다중 검증 방식을 적용했다.
- **Result** :
    - **감지율 대폭 향상**: Scapy 기반 능동적 스캔과 재시도 로직을 통해 이전에 미탐지되던 iOS 기기와 절전 모드 Android 기기의 감지율이 95% 이상으로 향상되었다.
    
    ![image.png](attachment:0469a3cd-7ae7-4e48-bb39-2f36f77886c0:image.png)
    
    ▲ 8개의 로컬 네트워크 기기중 3개만 탐지하고 있는 상태
    
    ![image.png](attachment:e94e0b7d-4a00-4b2c-a63e-fae41a88bbbf:image.png)
    
    ▲ 모든 기기가 로컬 네트워크 기기가 탐지되고 있는 상태
    
    - **침입 오탐 제거**: 사용자가 집에 있는 상황에서 "침입 감지" 경보가 발생하는 오작동이 거의 사라졌으며, 다중 기기 등록을 통해 시스템의 신뢰성이 크게 향상되었다.
    - **안정적인 재실 판단**: 네트워크 스캔 결과가 일관성 있게 유지되어, 화재 발생 시 "재실 여부"와 결합한 정확한 위험도 판단이 가능해졌다.

## 4.2. 낙상 및 쓰러짐 탐지

### 4.2.1. 대상 및 낙상 오탐지 문제

- **Problem** :
    - **수면 vs 낙상 혼동**: 초기에는 단순히 누워있는 자세(Lying Pose)를 낙상으로 판단했으나, 이는 정상적인 수면 상태와 구분이 되지 않아 오경보가 발생했다.
    - **사물 오인식**: 옷걸이에 걸린 옷, 의자, 가구 등 형태가 유사한 사물을 사람으로 잘못 인식하여 낙상 경보가 울리는 False Positive가 발생했다.
- **Solution** :
    - Keypoint 가 y축으로 낙하할 시에만 낙상으로 판별하도록 알고리즘을 개선하여 동일한 누워있는 frame 이더라도 누워있는 것과 낙상을 구분할 수 있도록 하였다.
    - YOLOv5-Pose 모델은 사람의 keypoint(관절점) 구조를 검출하므로, 사물과 달리 머리-어깨-골반으로 이어지는 skeleton topology가 명확히 감지되어야 사람으로 인식된다. Detection confidence threshold(> 0.5)를 적용하여 모호한 검출을 제외하고, 신뢰도 높은 사람만 분석 대상으로 포함시켰다.
- **Result** :
    - 수면과 낙상을 구분하여, 동일한 누워있는 상태이더라도 낙상에 대해서만 정상적으로 경보를 울린다.
        - e.g. 누워있는 형태를 낙상으로 인식하지 않음.
            
            ![스크린샷 2025-12-01 184927.png](attachment:04d7615f-cbdf-4039-93b0-81ce4548e6f0:스크린샷_2025-12-01_184927.png)
            
    - 사물이 아닌 사람만을 인식하여 낙상을 탐지할 수 있다.
        - e.g. 옷과 의자를 대상으로 인식하지 않음.
            
            ![image.png](attachment:e721e551-d95a-4e4f-a328-46d81f38cdb2:image.png)
            
            [fall_detection_cloth (online-video-cutter.com).mp4](attachment:97bacbce-37e5-421e-a709-38a67ab6dfa9:fall_detection_cloth_(online-video-cutter.com).mp4)
            

### 4.2.2. 가림 현상(Occlusion) 및 인식 실패

- **Problem** :
    - **이불 가림**: 수면 중 이불을 덮고 있을 경우 신체가 가려져 관절 인식(YOLOv5-Pose)이 불가능했다.
    - **사각지대**: 옷장이나 가구 등 집안 사물에 의해 사람이 부분적으로 가려질 경우 일부 관절 마디를 파악하지 못하는 경우가 있었다.
- **Solution** :
    - **High-Angle 설치**: CCTV 설치 각도를 일반적인 눈높이보다 높은 천장 부근으로 조정하여 사각지대를 최소화하고 관절 인식률을 개선했다.
    - **부분 가림 대응**: YOLOv5-Pose는 가려진 부분의 keypoint를 학습된 포즈 구조(skeleton topology)를 기반으로 추정할 수 있다. 예를 들어, 어깨와 골반이 보이면 가려진 팔꿈치 위치를 유추 가능하다.
    - **우선순위 기반 머리 추적**: 코(nose) → 눈(eyes) → 귀(ears) 순서로 감지 가능한 keypoint를 자동 선택하여, 일부가 가려져도 머리 위치를 추적할 수 있도록 구현했다.
    - **낮은 confidence threshold 허용**: 가림 상황에서도 낙상 감지가 가능하도록, 개별 keypoint의 confidence threshold를 낮게 설정(0.1~0.3)하여 부분적으로 보이는 관절도 활용하도록 했다.
- **Result** : 장애물 및 이불 등 물체로 일부 가려져도 주요 keypoint를 추정하여 쓰러짐을 감지할 수 있도록 기능을 완성하였다.
    - e.g. 이불을 덮고 다리를 편 경우 ⇒ 일자 다리로 인식
        
        ![image.png](attachment:fd22fca4-fe6d-456e-bec5-87c614571e35:b9fa4fd5-6501-4e6d-90b5-624bc7b559c3.png)
        
    - e.g. 이불을 덮고 다리를 구부린 경우 ⇒ 구부린 다리로 인식
        
        ![image.png](attachment:18f100e2-ce8f-45cc-ad1a-da5a31522c20:image.png)
        
    

### 4.2.3. 원근감 왜곡 문제

- **Problem** : 일반적인 CCTV 환경을 모사하여 **벽면 상단에서 비스듬히 내려다보는 High-Angle**로 카메라를 설치하고, 초기 YOLOv7 기반의 Bounding Box 정중앙 좌표(Center Point)의 Y축 변화량을 추적하였을 때, **원근감 왜곡** 문제가 발생하여 사람이 걸어가거나 단순히 이동하는 동작임에도 좌표 변화가 감지되어 낙상으로 오인하는 문제가 발생했다.
- **Solution (Pose Model 전환 + 상대적 비율 분석)** :
    - 단순 박스 영역이 아닌, 신체 관절의 구조적 연결성(Skeleton)을 파악하는 **YOLOv5-Pose** 모델로 전환하였다.
    - 단순한 좌표 이동이 아니라 **머리, 어깨, 골반 등 주요 Keypoint 간의 관계**를 분석함으로써 입체적인 움직임을 추론할 수 있도록 개선했다.
        - **상대적 비율 기반 분석**: 절대 좌표 대신 원근법에 강건한 상대적 메트릭을 활용했다.
            - `aspect_ratio = 머리-허리 거리 / 가로 확장도` (세로/가로 비율)
            - 정상 자세: 비율 > 1.5 (세로가 가로보다 길다)
            - 넘어짐: 비율 < 0.65 (가로가 세로보다 훨씬 길다)
            - 이 비율은 카메라와의 거리에 무관하게 일정하게 유지된다.
- **Result** : 2D 이미지의 한계인 원근 왜곡 상황에서도 실제 사람의 자세가 무너지는지를 정확히 판단할 수 있게 되었으며, 단순 이동/앉기 동작과 낙상을 명확히 구분하는 로직을 확보했다.

## 4.3. 수면 관리

### 4.3.1 카메라 각도에 따른 자세 오분류 문제

- Problem : 초기에 일반적인 정면 뷰를 가정하고 ‘양 어깨의 수평(X축) 너비’를 기준으로 자세 분류를 하려했다. 그러나 CCTV와 같은 비스듬한 상단에 카메라를 설치하자 원근 왜곡으로 인해 정자세일 때도 어깨 너비가 좁게 측정되어 모든 자세가 ‘측면(Side)’로 오분류되는 현상이 발생했다.
- Solution : 카메라 시점에서는 사람이 누웠을 때(정자세)는 어깨 높이가 비슷하지만, 옆으로 누웠을 때(측면)는 어깨가 수직으로 쌓여 높이 차이가 발생함에 착안했다. 판별 기준을 ‘X축 너비’에서 양 어깨의 Y축 좌표 차이’로 변경하고, 실측 데이터를 기반으로 임계값(30px)을 재설정하였다.
- Result : 다양한 수면 자세에서도 수면 자세를 95% 이상 정확하게 구분하는 데 성공했다.

### 4.3.2 노이즈로 인한 뒤척임 오탐지

- Problem : NPU 모델 특성상 사람이 가만히 있어도 관절 좌표가 미세하게 떨리는 Jitter 현상이 발생했다. 또한, 이불에 의해 팔다리가 가려지면 관절 위치가 튀는 현상이 있어, 실제 뒤척임이 없는데도 카운트가 증가하는 문제가 있었다.
- Solution :
    1. Target 변경 : 불안정한 팔다리 관절 대신, 신체의 무게 중심인 ‘몸통(어깨 + 엉덩이 중심)’ 좌표만을 추적 대상으로 변경했다.
    2. Threshold Filtering : 프레임 간 이동 거리가 15픽셀 이하인 미세한 움직임은 노이즈로 간주하여 무시하고, 이를 초과하는 큰 움직임만 카운트하도록 필터링 로직을 적용했다.
- Result : 수면 중 발생할 수 있는 미세한 움직임은 무시하고, 실제 수면 환경을 저해하는 큰 뒤척임만을 정확히 감지하게 되었다.

### 4.3.3 데이터 보안 및 전송 지연

- Problem : 초기에는 HiveMQ 공용 클라우드 브로커를 사용하여 데이터를 전송했다. 이로 인해 외부 인터넷망 상태에 따라 전송 지연(Latency)이 발생했고, 민감한 수면 데이터가 외부 서버를 경유한다는 프라이버시 보안 이슈가 존재했다.
- Solution : 외부 클라우드 의존성을 완전히 제거하기 위해, 제어용 PC에 Mosquitto 로컬 브로커를 직접 구축하여 Orange Pi와 PC가 로컬 공유기 내부망에서만 통신하도록 네트워크 아키텍처를 변경했다.
- Result : 데이터 전송 속도가 향상되고, 데이터가 외부로 유출되지 않는 완벽한 프라이버시 보호 환경을 구현했다.
    
    ![스크린샷 2025-12-03 000947.png](attachment:90aed578-53f8-4809-ba1f-d78d5248f63f:스크린샷_2025-12-03_000947.png)
    

### 4.3.4 이불 속 자세 탐지

- Problem : 신체 가림(Occlusion) 현상 - YLOv5-Pose 모델 특성 상 이불을 덮으면 주요 관절이 가려져 신뢰도 급격히 하락
좌표 노이즈로 인해 엎드린 자세를 측면으로 오분류하거나, 상태가 수시로 바뀌는 Flickering 현상 발생
- Solution : 관절 신뢰도에 따라 어깨 좌표 기울기만이 아닌 머리 종합 신뢰도까지 관찰하여 정상 분류, 단일 프레임이 아닌 최근 프레임 결과를 분석하여 일시적인 노이즈 필터링
- Result : 이불을 덮은 상태에서도 Upright/Side/Prone 자세를 명확히 구분하여 안정적인 모니터링 데이터 확보

[sleep_monitor_2.mp4](attachment:1834dcdb-9eb6-43ef-86a3-12d87705631b:sleep_monitor_2.mp4)

### 4.3.4 유의미한 뒤척임 감지

- Problem : 단순히 침대에 누워있거나 잠들기 전 뒤척이는 움직임까지 모두 카운트되어, 실제 수면 효율 분석의 정확도가 떨어지는 문제 발생
- Solution : NPU가 모든 움직임을 카운트하지 않고, HA로 동기화된 상태가 ‘DEEP_SLEEP(숙면 모드)’일 때의 움직임만 유효한 데이터로 처리하도록 조건부 로직 구현
- Result : 깨어있을 때의 노이즈 데이터를 제거하고, 실제 수면 질에 영향을 미치는 뒤척임만 정밀하게 기록하여 스마트홈 제어의 신뢰도 상승

# [5] 결론

본 프로젝트는 급증하는 1인 가구와 독거노인의 생활 안전 및 건강 관리를 위해 DeepX NPU를 활용한 온디바이스(On-Device) AI 시스템인 ‘Monoculus’를 구현하였다. 본 시스템은 **YOLOv5-Pose, YOLOv7, YOLOv8** 등 목적에 최적화된 3가지 AI 모델을 상황에 따라 유기적으로 전환/병렬 운용하는 지능형 아키텍처를 채택하였다. 이를 통해 실시간 비전 분석과 Home Assistant 기반의 능동형 IoT 제어, 그리고 모바일 앱을 통한 시각화까지 통합된 올인원 솔루션을 완성하였다.

주요 구현 내용은 다음과 같다.

1. 시스템 아키텍처 : 사용자의 재실 여부(ARP/MAC 감지)에 따라 시스템이 자동으로 **재실 모드 (Home)**와 **외출 모드(Away)**로 전환되며, 각 모드에 최적화된 AI 모델을 NPU에 로드하여 효율성을 극대화하였다.
2. 안전 관리(Safety) : 
    - 외출 모드 (Away Mode) : 보안에 특화된 **YOLOv8(침입 탐지)**과 **YOLOv7(화재 탐지)** 모델을 NPU 상에서 동시에 구동하여, 외부인의 침입과 화재 징후를 실시간으로 감시한다.
    - 재실 모드 (Home Mode) :  **YOLOv5-Pose** 모델로 전환하여 사용자의 스켈레톤(관절)을 추출하고, 신체의 가속도와 비율 변화를 분석하는 낙상 알고리즘을 통해 응급 상황을 즉시 감지한다.
3. 수면 케어 (Health) : 침실의 상단 뷰 환경에 특화된 자세 판별 로직과 빈도 기반 뒤척임 감지 기술을 개발하여 수면 상태를 정밀 분석하고, 냉난방기(HVAC)를 자동으로 제어하는 능동형 수면 관리 파이프라인을 완성하였다.
4. 시스템 통합 (Integration) : 모든 데이터는 외부 클라우드가 아닌 로컬 MQTT 통신을 통해 처리되며, 자체 개발한 모바일 애플리케이션이 모든 데이터 루프를 시각화하여 사용자가 수면 자세, 실내 온도, 보안 상태 등을 직관적으로 모니터링하고, 감지부터 제어까지의 전 과정이 인터넷 연결 없이도 독립적으로 수행되는 안정적인 엣지 시스템을 구현하였다.

‘Monoculus’ 시스템은 기존의 단순 모니터링 방식이 가진 한계를 기술적으로 극복하고, 사회적 안전망 구축에 기여한다는 점에서 다음과 같은 중요한 의의와 기대효과를 갖는다.

1. 온디바이스 (On-Device) 처리를 통한 프라이버시 보호의 새로운 표준 제시
    
    기존의 IP 카메라나 클라우드 기반 AI 서비스는 영상 데이터의 외부 유출 위험으로 인해 침실과 같은 사적인 공간에 도입되기 어려웠다. 본 프로젝트는 DeepX NPU를 활용하여 모든 AI 연산을 엣지 디바이스 내부에서 처리하고, 익명화된 텍스트 데이터(상태 정보)만을 전송하는 구조를 구현하였다. 이는 사용자의 사생활 침해 우려를 원천적으로 차단함으로써, 스마트홈 보안 기술의 신뢰성을 높이고 가정 내 AI 도입 장벽을 획기적으로 낮출 것으로 기대된다.
    
2. 골든타임 확보를 통한 사회적 안전망 강화
1인 가구, 특히 고령층의 경우 낙상 사고나 화재 발생 시 초기 대응이 생존율을 결정한다. 본 시스템은 NPU의 고속 병렬 연산을 통해 사고를 실시간으로 감지하고 즉시 경보를 울림으로써, 인지 지연 없이 골든타임을 확보할 수 있다. 이는 고독사 예방 및 사회적 비용 절감에 기여할 수 있는 실질적인 안전 솔루션이 될 것이다.
3. 수동적 감시에서 능동적 케어로의 패러다임 전환
    
    기존의 웨어러블 기기나 수면 앱은 단순히 수면 데이터를 기록하는 데 그쳤다. 그러나 Monoculus는 사용자의 뒤척임과 수면 자세를 분석한 후, 이를 기반으로 냉난방기(HVAC)를 자동으로 제어하여 최적의 수면 환경을 조성한다. 이는 AI가 데이터를 수집하는 단계를 넘어, 물리적 환경을 제어하여 사용자의 삶의 질을 실질적으로 개선하는 능동형 헬스케어의 가능성을 입증하였다.
    
4. 엣지 AI의 상용화 가능성 및 확장성 확인
    
    고가의 GPU 서버 없이 저전력 NPU만으로도 복잡한 비전 AI와 IoT 제어가 가능함을 확인하였다. 이 경량화된 시스템 구조는 향후 멀티모달(음성+영상) 분석이나 단일 CCTV 올인원 통합 등 기능확장이 용이하며, 독거노인 돌봄 서비스, 병실 모니터링, 스마트 요양원 등 다양한 B2B/B2G 헬스케어 분야로 저비용·고효율 확장이 가능할 것으로 전망된다.
    

# [6] 향후 개선 방향

 본 프로젝트는 DeepX NPU를 활용한 엣지 기반의 실시간 낙상 감지 시스템을 성공적으로 구축하였다. 향후 본 시스템을 단순한 감지기를 넘어 상용화 가능한 수준의 ‘통합 라이프케어 솔루션’으로 발전시키기 위해 다음과 같은 4단계 고도화 방안을 제안한다.

## 6.1. 성능 개선

- **배경**: 초기 단계에서는 위험 상황을 놓치지 않기 위해 재현율(Recall)을 높이는 데 주력했으나, 이는 잦은 오경보(False Alarm)를 유발하여 사용자의 피로도를 높일 수 있다. 또한, 4.2.2에서 다룬 가림 현상(Occlusion)에 대한 대응이 더욱 정교해질 필요가 있다.
- **개선**: 노이즈를 제거하는 알고리즘인 칼만 필터(Kalman Filter) 또는 ByteTrack 알고리즘을 도입하여 관절 좌표의 노이즈(Jitter)를 보정하고, 단순 이동이 아닌 궤적(Trajectory)의 연속성을 분석하여 오탐지를 줄인다.

## 6.2. 온디바이스 멀티모달(Multimodal) 시스템 확장

- **배경**: 시각 정보(CCTV)만으로는 상황의 맥락을 완벽히 이해하거나, 시야 밖의 위험을 감지하는 데 한계가 있다. 이를 보완하기 위해 청각 및 텍스트 데이터를 융합한다.
- **개선**:
    - 음성(Audio) 융합: 마이크 센서를 추가하여 비명 소리, ‘쿵’ 하는 충격음, 화재 경보기 소리, 코골이 소리를 딥러닝으로 분석한다. 영상 데이터와 소리 데이터를 결합(Fusion)하여 화재 및 낙상 판별의 정확도를 높이고, 수면 무호흡이나 코골이 분석을 통해 수면 관리의 질을 향상시킨다.
    - Edge-VLM 기반 상황 브리핑: 경량화된 시각-언어 모델(Edge-VLM, eg. Phi-3-vision)을 NPU에 탑재한다. 낙상 발생 시 단순 경보를 넘어, "사용자가 침대에서 일어나려다 이불에 발이 걸려 넘어짐"과 같이 영상 정보를 요약된 줄글 형태(Text Briefing)로 변환하여 보호자에게 제공한다. 이 모든 과정은 엣지 디바이스 내부에서 처리되어 프라이버시를 보호한다.

## 6.3. 프라이버시 보호 애플리케이션 구축

- **배경**: 1인 가구 특성상 현장에 보호자가 없으므로 즉각적인 알림이 필수적이다. 그러나 일반적인 IoT 앱은 서버로 영상을 전송하므로 해킹 및 사생활 유출의 위험이 존재한다.
- **개선**: 온디바이스 AI의 장점을 극대화한 ‘서버리스(Serverless) 지향 보안 통신 앱’을 개발한다.
    - 이벤트 트리거 데이터 전송: 평상시에는 어떠한 데이터도 외부로 전송하지 않는다. 오직 NPU가 위험을 감지한 순간에만 네트워크를 활성화하여 텍스트(시간, 위치, 상황 요약) 정보를 우선 전송한다.
    - P2P 영상 터널링 (WebRTC): 보호자가 현장 확인을 요청할 경우, 영상을 클라우드 서버에 저장하지 않고 WebRTC P2P 기술을 통해 CCTV와 보호자 스마트폰을 1:1로 직접 연결한다. 이는 영상 데이터가 중앙 서버를 거치지 않으므로, 서버 해킹 시에도 사용자의 영상 데이터가 유출될 원천적인 가능성을 차단한다.
    - 골든타임 대응: 위 보안 기술을 바탕으로 영상 유출 걱정 없이 안심하고 사용할 수 있는 119/보호자 자동 신고 시스템을 완성한다.
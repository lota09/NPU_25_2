"""
YOLOv7 í™”ì¬ ê°ì§€ ëª¨ë¸ ë™ì˜ìƒ ë¹„êµ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
ë‘ ê°œì˜ ëª¨ë¸ì„ ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ë™ì˜ìƒìœ¼ë¡œ í‰ê°€í•˜ì—¬ ê³µì •í•˜ê²Œ ë¹„êµ
ê²°ê³¼ ë™ì˜ìƒì„ ìƒì„±í•˜ì—¬ ìœ¡ì•ˆìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥
"""

import cv2
import torch
import numpy as np
from pathlib import Path
import time
import sys

# YOLOv7 ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / 'yolov7'))

from models.experimental import attempt_load
from utils.general import non_max_suppression, scale_coords
from utils.plots import plot_one_box
from utils.torch_utils import select_device
from utils.datasets import letterbox

def check_gpu_status():
    """GPU ìƒíƒœ í™•ì¸"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ–¥ï¸  GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        print(f"âœ… CUDA: {torch.version.cuda}")
        return True
    else:
        print("âš ï¸  GPU ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œ")
        return False

def load_yolov7_model(weights_path, device):
    """YOLOv7 ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ¤– ëª¨ë¸ ë¡œë”©: {weights_path}")
    model = attempt_load(weights_path, map_location=device)
    model.eval()
    print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    # GPU ìµœì í™”
    if device.type != 'cpu':
        model.half()  # FP16
        print(f"   âœ… FP16 ìµœì í™” ì ìš©")
        
    return model

def process_video_yolov7(video_path, model, device, output_path, model_name, conf_thres=0.5, iou_thres=0.45, img_size=640, class_conf_thres=None):
    """YOLOv7ë¡œ ë™ì˜ìƒ ì²˜ë¦¬ ë° ê²°ê³¼ ì €ì¥
    
    Args:
        class_conf_thres: í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ì„ê³„ê°’ dict (ì˜ˆ: {'flame': 0.7, 'smoke': 0.3})
    """
    cap = cv2.VideoCapture(str(video_path))
    
    if not cap.isOpened():
        print(f"âŒ ë™ì˜ìƒ ì—´ê¸° ì‹¤íŒ¨: {video_path}")
        return None
    
    # ë™ì˜ìƒ ì •ë³´
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"   ğŸ“º {width}x{height} | {fps} FPS | {total_frames} frames")
    
    # ë¹„ë””ì˜¤ ë¼ì´í„° ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
    
    # ê²°ê³¼ ì €ì¥ìš©
    detections = []
    inference_times = []
    frame_count = 0
    
    half = device.type != 'cpu'
    
    # í´ë˜ìŠ¤ ì´ë¦„ (ë°ì´í„°ì…‹ ì„¤ì •ê³¼ ì¼ì¹˜: 0=flame, 1=smoke)
    names = ['flame', 'smoke']
    
    # ìƒ‰ìƒ (flame: ë¹¨ê°•, smoke: íšŒìƒ‰)
    colors = [(0, 0, 255), (128, 128, 128)]
    
    # í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ ì„ê³„ê°’ ì„¤ì •
    if class_conf_thres is None:
        class_conf_thres = {'flame': 0.5, 'smoke': 0.5}  # ê¸°ë³¸ê°’: ë¶ˆê½ƒ ì—„ê²©, ì—°ê¸° ê´€ëŒ€
    print(f"   ğŸ¯ í´ë˜ìŠ¤ë³„ ì„ê³„ê°’: flame={class_conf_thres.get('flame', conf_thres)}, smoke={class_conf_thres.get('smoke', conf_thres)}")
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # ì „ì²˜ë¦¬ (ì¢…íš¡ë¹„ ìœ ì§€í•˜ë©° ë¦¬ì‚¬ì´ì¦ˆ - letterbox)
            img = letterbox(frame, img_size, stride=32, auto=True)[0]
            img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, HWC to CHW
            img = np.ascontiguousarray(img)
            img = torch.from_numpy(img).to(device)
            img = img.half() if half else img.float()
            img /= 255.0
            if img.ndimension() == 3:
                img = img.unsqueeze(0)
            
            # ì¶”ë¡ 
            t0 = time.time()
            with torch.no_grad():
                pred = model(img)[0]
            inference_time = (time.time() - t0) * 1000  # msë¡œ ë³€í™˜
            inference_times.append(inference_time)
            
            # NMS ì ìš©
            pred = non_max_suppression(pred, conf_thres, iou_thres)
            
            # ê°ì§€ ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
            det = pred[0]
            if len(det):
                # ì¢Œí‘œ ìŠ¤ì¼€ì¼ ì¡°ì •
                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()
                
                # í´ë˜ìŠ¤ë³„ ì‹ ë¢°ë„ í•„í„°ë§ ì ìš©
                filtered_det = []
                for *xyxy, conf, cls in det:
                    cls_idx = int(cls)
                    cls_name = names[cls_idx]
                    cls_threshold = class_conf_thres.get(cls_name, conf_thres)
                    
                    # í´ë˜ìŠ¤ë³„ ì„ê³„ê°’ í†µê³¼í•œ ê²ƒë§Œ ìœ ì§€
                    if conf >= cls_threshold:
                        filtered_det.append([*xyxy, conf, cls])
                
                # í•„í„°ë§ëœ ê²°ê³¼ë¡œ ëŒ€ì²´
                if filtered_det:
                    det = torch.tensor(filtered_det).to(det.device)
                    
                    detections.append({
                        'frame': frame_count,
                        'count': len(det),
                        'confidences': det[:, 4].cpu().numpy()
                    })
                    
                    # ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                    for *xyxy, conf, cls in reversed(det):
                        label = f'{names[int(cls)]} {conf:.2f}'
                        plot_one_box(xyxy, frame, label=label, color=colors[int(cls)], line_thickness=2)
            
            # ëª¨ë¸ ì´ë¦„ê³¼ í”„ë ˆì„ ì •ë³´ ì¶”ê°€
            cv2.putText(frame, f'Model: {model_name}', (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(frame, f'Frame: {frame_count}/{total_frames}', (10, 70), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            if len(det) if isinstance(det, torch.Tensor) else False:
                cv2.putText(frame, f'Detections: {len(det)}', (10, 110), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            # í”„ë ˆì„ ì €ì¥
            out.write(frame)
            
            # ì§„í–‰ë¥  í‘œì‹œ
            if frame_count % 30 == 0:
                progress = (frame_count / total_frames) * 100
                avg_inference = np.mean(inference_times[-30:])
                print(f"   ğŸ“Š {progress:.1f}% | âš¡ {avg_inference:.1f}ms", end='\r')
        
        print()  # ì¤„ë°”ê¿ˆ
        
    finally:
        cap.release()
        out.release()
    
    # í†µê³„ ê³„ì‚°
    stats = {
        'total_frames': total_frames,
        'processed_frames': frame_count,
        'avg_inference_ms': np.mean(inference_times),
        'total_detections': len(detections),
        'avg_confidence': np.mean([np.mean(d['confidences']) for d in detections]) if detections else 0,
        'detection_rate': len(detections) / frame_count * 100 if frame_count > 0 else 0
    }
    
    return stats

def compare_models_on_videos(model1_path, model2_path, video_paths, output_dir='results/model_comparison'):
    """ë‘ ëª¨ë¸ì„ ì—¬ëŸ¬ ë™ì˜ìƒìœ¼ë¡œ ë¹„êµ ë° ê²°ê³¼ ë™ì˜ìƒ ìƒì„±"""
    
    print("ğŸ”¥ YOLOv7 í™”ì¬ ê°ì§€ ëª¨ë¸ ë¹„êµ í‰ê°€")
    print("=" * 70)
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # GPU í™•ì¸
    gpu_available = check_gpu_status()
    device = select_device('0' if gpu_available else 'cpu')
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ“¦ ëª¨ë¸ 1 ë¡œë”©...")
    model1 = load_yolov7_model(model1_path, device)
    model1_name = Path(model1_path).parent.parent.name
    
    print(f"\nğŸ“¦ ëª¨ë¸ 2 ë¡œë”©...")
    model2 = load_yolov7_model(model2_path, device)
    model2_name = Path(model2_path).parent.parent.name
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ†š ëª¨ë¸ ë¹„êµ: {model1_name} vs {model2_name}")
    print("=" * 70)
    
    # ê° ë™ì˜ìƒìœ¼ë¡œ í‰ê°€
    results = {
        'model1': {'name': model1_name, 'videos': {}},
        'model2': {'name': model2_name, 'videos': {}}
    }
    
    for video_path in video_paths:
        video_name = Path(video_path).name
        video_stem = Path(video_path).stem
        print(f"\nğŸ¬ í…ŒìŠ¤íŠ¸ ë™ì˜ìƒ: {video_name}")
        print("-" * 70)
        
        # ì¶œë ¥ ê²½ë¡œ ì„¤ì •
        output1 = output_dir / f"{video_stem}_{model1_name}.mp4"
        output2 = output_dir / f"{video_stem}_{model2_name}.mp4"
        
        # ëª¨ë¸ 1 í‰ê°€ ë° ë™ì˜ìƒ ìƒì„±
        if output1.exists():
            print(f"   â­ï¸  {model1_name} ê²°ê³¼ ë™ì˜ìƒì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {output1.name}")
            stats1 = None
        else:
            print(f"   âš¡ {model1_name} í‰ê°€ ë° ë™ì˜ìƒ ìƒì„± ì¤‘...")
            stats1 = process_video_yolov7(video_path, model1, device, output1, model1_name)
            print(f"   ğŸ’¾ ì €ì¥: {output1}")
        results['model1']['videos'][video_name] = stats1
        
        # ëª¨ë¸ 2 í‰ê°€ ë° ë™ì˜ìƒ ìƒì„±
        if output2.exists():
            print(f"   â­ï¸  {model2_name} ê²°ê³¼ ë™ì˜ìƒì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤: {output2.name}")
            stats2 = None
        else:
            print(f"   âš¡ {model2_name} í‰ê°€ ë° ë™ì˜ìƒ ìƒì„± ì¤‘...")
            stats2 = process_video_yolov7(video_path, model2, device, output2, model2_name)
            print(f"   ğŸ’¾ ì €ì¥: {output2}")
        results['model2']['videos'][video_name] = stats2
        
        # ë™ì˜ìƒë³„ ë¹„êµ ì¶œë ¥ (ë‘˜ ë‹¤ ì²˜ë¦¬ëœ ê²½ìš°ë§Œ)
        if stats1 and stats2:
            print(f"\n   ğŸ“Š {video_name} ê²°ê³¼:")
            print(f"   {'í•­ëª©':<20} {model1_name:<25} {model2_name:<25}")
            print(f"   {'-'*20} {'-'*25} {'-'*25}")
            print(f"   {'ì¶”ë¡  ì‹œê°„ (ms)':<20} {stats1['avg_inference_ms']:<25.1f} {stats2['avg_inference_ms']:<25.1f}")
            print(f"   {'ì´ ê°ì§€ íšŸìˆ˜':<20} {stats1['total_detections']:<25} {stats2['total_detections']:<25}")
            print(f"   {'ê°ì§€ìœ¨ (%)':<20} {stats1['detection_rate']:<25.1f} {stats2['detection_rate']:<25.1f}")
            print(f"   {'í‰ê·  ì‹ ë¢°ë„':<20} {stats1['avg_confidence']:<25.3f} {stats2['avg_confidence']:<25.3f}")
        elif stats1 or stats2:
            print(f"\n   â„¹ï¸  {video_name}: ì¼ë¶€ ê²°ê³¼ê°€ ê±´ë„ˆë›°ì–´ì¡ŒìŠµë‹ˆë‹¤.")
    
    # ì „ì²´ ìš”ì•½
    print("\n" + "=" * 70)
    # í‰ê·  ê³„ì‚° (Noneì´ ì•„ë‹Œ ê°’ë§Œ ì‚¬ìš©)
    model1_stats = [s for s in results['model1']['videos'].values() if s is not None]
    model2_stats = [s for s in results['model2']['videos'].values() if s is not None]
    
    if not model1_stats and not model2_stats:
        print("\nâš ï¸  ëª¨ë“  ê²°ê³¼ê°€ ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ìƒˆë¡œ ì²˜ë¦¬ëœ ë™ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ê¸°ì¡´ ê²°ê³¼ í™•ì¸: {output_dir.absolute()}")
        return results
    
    model1_avg_inference = np.mean([s['avg_inference_ms'] for s in model1_stats]) if model1_stats else 0
    model2_avg_inference = np.mean([s['avg_inference_ms'] for s in model2_stats]) if model2_stats else 0
    
    model1_total_det = sum([s['total_detections'] for s in model1_stats]) if model1_stats else 0
    model2_total_det = sum([s['total_detections'] for s in model2_stats]) if model2_stats else 0
    
    model1_avg_conf = np.mean([s['avg_confidence'] for s in model1_stats if s['avg_confidence'] > 0]) if model1_stats else 0
    model2_avg_conf = np.mean([s['avg_confidence'] for s in model2_stats if s['avg_confidence'] > 0]) if model2_stats else 0
    
    model1_avg_conf = np.mean([s['avg_confidence'] for s in results['model1']['videos'].values() if s['avg_confidence'] > 0])
    model2_avg_conf = np.mean([s['avg_confidence'] for s in results['model2']['videos'].values() if s['avg_confidence'] > 0])
    
    print(f"\n{'ì§€í‘œ':<25} {model1_name:<25} {model2_name:<25}")
    print(f"{'-'*25} {'-'*25} {'-'*25}")
    print(f"{'í‰ê·  ì¶”ë¡  ì‹œê°„ (ms)':<25} {model1_avg_inference:<25.1f} {model2_avg_inference:<25.1f}")
    print(f"{'ì´ ê°ì§€ íšŸìˆ˜':<25} {model1_total_det:<25} {model2_total_det:<25}")
    print(f"{'í‰ê·  ì‹ ë¢°ë„':<25} {model1_avg_conf:<25.3f} {model2_avg_conf:<25.3f}")
    
    # ìŠ¹ì íŒì •
    print(f"\n{'='*70}")
    print("ğŸ† ì¢…í•© í‰ê°€")
    print(f"{'='*70}")
    
    winner_speed = model1_name if model1_avg_inference < model2_avg_inference else model2_name
    winner_detection = model1_name if model1_total_det > model2_total_det else model2_name
    winner_confidence = model1_name if model1_avg_conf > model2_avg_conf else model2_name
    
    print(f"âš¡ ì†ë„ ìš°ìœ„: {winner_speed}")
    print(f"ğŸ¯ ê°ì§€ ì„±ëŠ¥ ìš°ìœ„: {winner_detection} (ë‹¨, ì •ë‹µ ë¼ë²¨ ì—†ì–´ ì°¸ê³ ë§Œ)")
    print(f"âœ… ì‹ ë¢°ë„ ìš°ìœ„: {winner_confidence}")
    
    print(f"\nğŸ“ ê²°ê³¼ ë™ì˜ìƒ ì €ì¥ ìœ„ì¹˜: {output_dir.absolute()}")
    print(f"   - ê° í…ŒìŠ¤íŠ¸ ë™ì˜ìƒë§ˆë‹¤ 2ê°œì˜ ê²°ê³¼ íŒŒì¼ ìƒì„±ë¨")
    print(f"   - ì´ {len(video_paths) * 2}ê°œì˜ ê²°ê³¼ ë™ì˜ìƒ")
    print(f"\nğŸ’¡ ì •ëŸ‰ì  í‰ê°€ëŠ” runs/*/results.txtì˜ mAP ê°’ì„ ì°¸ê³ í•˜ì„¸ìš”")
    
    return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # ëª¨ë¸ ê²½ë¡œ
    model1_path = "runs/v7_merged_100epoch_16batch/weights/best.pt"
    model2_path = "runs/v7_merged_200epoch_16batch/weights/best.pt"
    
    # í…ŒìŠ¤íŠ¸ ë™ì˜ìƒ ê²½ë¡œ
    video_paths = [
        "assets/bucket11.mp4",
        "assets/printer31.mp4",
        "assets/roomfire41.mp4"
    ]
    
    # ì¡´ì¬ í™•ì¸
    if not Path(model1_path).exists():
        print(f"âŒ ëª¨ë¸ 1ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model1_path}")
        return
    
    if not Path(model2_path).exists():
        print(f"âŒ ëª¨ë¸ 2ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model2_path}")
        return
    
    missing_videos = [v for v in video_paths if not Path(v).exists()]
    if missing_videos:
        print(f"âŒ ë™ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for v in missing_videos:
            print(f"   - {v}")
        return
    
    # ë¹„êµ ì‹¤í–‰
    results = compare_models_on_videos(model1_path, model2_path, video_paths)
    
    print(f"\nâœ… ë¹„êµ í‰ê°€ ì™„ë£Œ!")
    print(f"\nğŸ“º ê²°ê³¼ ë™ì˜ìƒì„ ì—´ì–´ì„œ ìœ¡ì•ˆìœ¼ë¡œ ì„±ëŠ¥ì„ ë¹„êµí•˜ì„¸ìš”:")
    print(f"   results/model_comparison/ í´ë”ì— 6ê°œì˜ ë™ì˜ìƒ íŒŒì¼")

if __name__ == "__main__":
    main()
